{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/practical/fast-DiT')\n",
    "import torch\n",
    "from diffusion import create_diffusion\n",
    "from diffusers.models import AutoencoderKL\n",
    "from accelerate import Accelerator\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from models import DiT_S_2\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    print(\"GPU not found. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_186600\\1563228436.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DiT(\n",
       "  (x_embedder): PatchEmbed(\n",
       "    (proj): Conv2d(4, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (t_embedder): TimestepEmbedder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (y_embedder): LabelEmbedder(\n",
       "    (embedding_table): Embedding(1001, 384)\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x DiTBlock(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=False)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (adaLN_modulation): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=384, out_features=2304, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer): FinalLayer(\n",
       "    (norm_final): LayerNorm((384,), eps=1e-06, elementwise_affine=False)\n",
       "    (linear): Linear(in_features=384, out_features=32, bias=True)\n",
       "    (adaLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=384, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "image_size = 256 #@param [256, 512]\n",
    "vae_model = \"stabilityai/sd-vae-ft-ema\" #@param [\"stabilityai/sd-vae-ft-mse\", \"stabilityai/sd-vae-ft-ema\"]\n",
    "latent_size = int(image_size) // 8\n",
    "checkpoints_dir = os.path.join(os.getcwd(), \"checkpoints\")\n",
    "checkpoint_filename = \"0750000.pt\"\n",
    "checkpoint_path = os.path.join(checkpoints_dir, checkpoint_filename)\n",
    "model_class = DiT_S_2(input_size=latent_size).to(device)\n",
    "vae = AutoencoderKL.from_pretrained(vae_model).to(device)\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model_class):\n",
    "    model = model_class\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    #print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "load_checkpoint(checkpoint_path,model_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute GNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gns(model, diffusion, dataloader):\n",
    "    \"\"\"\n",
    "    Compute Gradient Noise Scale (GNS) for a model using a given loss function.\n",
    "\n",
    "    :param model: torch.nn.Module, Model Instance\n",
    "    :param diffusion: diffusion object, Diffusion model instance with training_losses\n",
    "    :param dataloader: DataLoader, Data Loader\n",
    "    :param device: str, \"cpu\" or \"cuda\"\n",
    "    :return: float, GNS value\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.train()  \n",
    "    grads = []  # Storing gradients for each batch\n",
    "    total_grad = None  # Overall gradient\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)        \n",
    "        t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=device)\n",
    "        model_kwargs = dict(y=y)\n",
    "\n",
    "        # Cal loss\n",
    "        model.zero_grad()\n",
    "        loss_dict = diffusion.training_losses(model, x, t, model_kwargs)\n",
    "        loss = loss_dict[\"loss\"].mean()\n",
    "        loss.backward()  \n",
    "\n",
    "        # Extract the gradient of the current batch\n",
    "        batch_grad = torch.cat([p.grad.view(-1) for p in model.parameters() if p.grad is not None])\n",
    "        grads.append(batch_grad)\n",
    "\n",
    "        # Accumulate overall gradient\n",
    "        if total_grad is None:\n",
    "            total_grad = batch_grad.clone()\n",
    "        else:\n",
    "            total_grad += batch_grad\n",
    "\n",
    "    # Compute tr(Î£)\n",
    "    grads = torch.stack(grads, dim=0)  # The gradients of each batch are stacked, with the shape (num_batches, num_params)\n",
    "    grad_mean = grads.mean(dim=0)\n",
    "    noise_cov = ((grads - grad_mean).t() @ (grads - grad_mean)) / grads.size(0)\n",
    "    trace_sigma = torch.trace(noise_cov)\n",
    "\n",
    "    # Compute |G|^2\n",
    "    norm_g_squared = torch.norm(total_grad / len(dataloader)) ** 2\n",
    "\n",
    "    # return GNS\n",
    "    return trace_sigma.item() / norm_g_squared.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_186600\\1563228436.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ImageFolder(imagenet_dir, transform\u001b[38;5;241m=\u001b[39mdata_transforms)\n\u001b[0;32m     33\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# (batch_size, channels, height, width)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Compute GNS\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25414\\anaconda3\\envs\\DiT\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\25414\\anaconda3\\envs\\DiT\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25414\\anaconda3\\envs\\DiT\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\25414\\anaconda3\\envs\\DiT\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25414\\anaconda3\\envs\\DiT\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\Users\\25414\\anaconda3\\envs\\DiT\\lib\\threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_size = 256 #@param [256, 512]\n",
    "vae_model = \"stabilityai/sd-vae-ft-ema\" #@param [\"stabilityai/sd-vae-ft-mse\", \"stabilityai/sd-vae-ft-ema\"]\n",
    "latent_size = int(image_size) // 8\n",
    "checkpoints_dir = os.path.join(os.getcwd(), \"checkpoints\")\n",
    "checkpoint_filename = \"0750000.pt\"\n",
    "checkpoint_path = os.path.join(checkpoints_dir, checkpoint_filename)\n",
    "model_class = DiT_S_2(input_size=latent_size).to(device)\n",
    "vae = AutoencoderKL.from_pretrained(vae_model).to(device)\n",
    "\n",
    "\n",
    "# Setup accelerator\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "# Create model and diffusion\n",
    "model = DiT_S_2(input_size=latent_size).to(device)\n",
    "diffusion = create_diffusion(timestep_respacing=\"\")  # default 1000 steps\n",
    "\n",
    "# Load checkpoint (optional)\n",
    "if checkpoint_path:\n",
    "    model = load_checkpoint(checkpoint_path, model)\n",
    "\n",
    "# Setup ImageNet DataLoader\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "imagenet_dir = os.path.join(os.getcwd(), \"dataset\")\n",
    "dataset = ImageFolder(imagenet_dir, transform=data_transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "for inputs, targets in dataloader:\n",
    "    print(inputs.shape)  # (batch_size, channels, height, width)\n",
    "# Compute GNS\n",
    "gns_value = compute_gns(model, diffusion, dataloader)\n",
    "#print(f\"Gradient Noise Scale (GNS): {gns_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DiT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
